{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning problem can be divided into two sub-categories -- **`Classification`** and **`Regression`** problem.\n",
    "* **`Classification`** refers to the task of assigning labels *`(yes/no, True/False, Ham/Spam)`* to data samples belonging to different classes. \n",
    "\n",
    "It can either be a two-class classification problem (mostly using **sigmoid function**) or a multi-class classifcation problem (mostly using **softmax function**). \n",
    "\n",
    "* **`Regression`**, on the other hand, refers to the task of predicting continuous values (scalar) by depicting the relationship between dependent variables and various independent features.\n",
    "\n",
    "`Linear Regression`, therefore, performs the task to predict a dependent variable value (y) based on a given independent variable (x) in a linear line. To find the line, **Ordinary Least Squared Method (OLS)** is widely used for regression problem. \n",
    "\n",
    "For complex problems, **`multivariate regression / Polynomial regression / Regularization term`** is utilized to enhance performance while at the same time restraining loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will first look at some basic assumptions behind a robust linear regression model.\n",
    "\n",
    "We will discuss what the assumptions are, and, if those assumptions are not satisfied, possible ways to transfrom our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the famous Boston Housing Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.6</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.7</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.4</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.2</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target     CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    24.0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    21.6  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    34.7  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    33.4  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    36.2  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston_dataset = load_boston()\n",
    "\n",
    "boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "boston['Target'] = boston_dataset.target\n",
    "boston = boston[boston.columns[-1:].append(boston.columns[:-1])]\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No null value is detected. It's good to go!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.drop(columns=['Target'])\n",
    "y = boston['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions for Evaluating Model Performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machines learn by means of **`loss function`**. It’s a method of evaluating how well specific algorithm models the given data. \n",
    "\n",
    "To evaluate the performance of the regression model, various loss functions are introduced and are preferred in different scenario.\n",
    "\n",
    "There’s no one-size-fits-all loss function to all the machine learning problems. Each loss function performs well in respective use cases in detecting different model performances, and should be considered before implementing one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='loss_all.png' width=\"400\" height=\"150\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, we are going to dive deep into some of the most common loss functions as below.\n",
    "\n",
    "* **`MSE`** : Mean Squared Error\n",
    "* **`RMSE`**: Root Mean Squared Error\n",
    "* **`MAE`** : Mean Absolute Error\n",
    "* **`MBE`** : Mean Bias Error\n",
    "* **`MAPE`**: Mean Absolute Percentage Error\n",
    "* **`R^2`** : R-Squared + Adj. R-Squared\n",
    "* **`RMSLE`** : Root Mean Squared Logarithmic Error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE / Quadratic Loss / L2 Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mse.png\" width=\"300\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Mean Squared Error (MSE)`** is one of the most simple and common loss function in regression analysis. For each data point, it calculates the squared difference between the prediction `ŷ` and original data point `y` and then take the mean value of those values.\n",
    "\n",
    "Due to the squared term, predictions that are far away from the actual values, which leads to overestimation of how bad the model is, are penalized compared to less deviated predictions. \n",
    "\n",
    "`MSE` is preferred over other other metrics such as `MAE`, because it is **differentiable** and hence can be optimized better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommended Use Cases** :\n",
    "* **`When Large Errors are undesirable`**:\n",
    "Since the errors are squared before they are averaged, the MSE penalizes even a small error which leads to over-estimation of how bad the model is. Therefore, if outliers are undesirable and should be cared about, use `MSE` to detect those large errors!\n",
    "\n",
    "\n",
    "* **`Further Calculation`**:\n",
    "MSE is also widely used due to its differentiable nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disadvantage** : \n",
    "* **`Overestimate the problem of a bad model`** : `MSE` is easily affected by outliers. A huge `MSE` often means that there are outliers.\n",
    "\n",
    "\n",
    "* **`Low Score may imply Overfitting`** : A low `MSE` does not imply good model, as it may be an overfitting model that fits all the data point.\n",
    "\n",
    "\n",
    "* **`Noisiness`** : If we have noisy data (that is, data includes some randomness or for whatever reason is not entirely reliable) — even a “perfect” model may have a high `MSE` in that situation, so it becomes hard to judge how well the model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(pred, y_test):\n",
    "    sum_err = 0.0\n",
    "    y = y_test.values\n",
    "    for i in range(len(y_test)):\n",
    "        err = pred[i] - y[i]\n",
    "        sum_err += (err**2)\n",
    "    return(sum_err / float(len(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.940057594323065"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(lr_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rmse.png\" width=\"300\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Squared Error is simply the root of the MSE. Then why bother to create another loss function?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `MSE` and `RMSE` decrease monotonically. Thus, a model that has a higher MSE will also have a higher RMSE compared to another model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate root mean squared error\n",
    "def rmse(pred, y_test):\n",
    "    sum_err = 0.0\n",
    "    y = y_test.values\n",
    "    for i in range(len(y_test)):\n",
    "        err = pred[i] - y[i]\n",
    "        sum_err += (err**2)\n",
    "    return(np.sqrt(sum_err / float(len(pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.379596415561586"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(lr_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error (MAE / L1 Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mae.png\" width=\"300\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Mean Absolute Error (MAE)`** : `MAE` measures the average magnitude of absolute differences in a set of prediction `ŷ` and its partnered data point `y`, without considering the direction. Unlike `RMSE` and `MSE`, `MAE` is more robust to extreme values since it doesn't have the squared term that penalizes errors as extremely as `MSE` does. That is, all the individual differences are weighted equally.\n",
    "\n",
    "**`Mean Absolute Error`** is widely used in cases like finance, where `$10`  error is usually exactly two times worse than `$5` error. \n",
    "\n",
    "On the other hand, `MSE` metric thinks that `$10` error is four times worse than `$5` error. \n",
    "Therefore, `MAE` is easier to justify than `MSE`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommended Use Cases** :\n",
    "* **`Pay equal attention to both all data points`**:\n",
    "Unlike `MSE` that pays higher emphasis on outliers, since `MAE` doesn't include the squared term, it is **suitable** for applications where you want to **pay equal attention to all the data points**. Therefore, if you **have outliers and want to treat it equally**, use `MAE`!\n",
    "\n",
    "\n",
    "* **`Easy comparison between differences`**:\n",
    "The `MAE` is a linear score which means that all the individual differences are weighted equally in the average. For example, the difference between `10` and `0` will be twice the difference between `5` and `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disadvantage** : \n",
    "* **`Hard to compute further`** : `MAE` is hard to compute further due to its absolute function. \n",
    "\n",
    "Meanwhile, its gradient either `-1` or `1`. In worst scenarios when `ŷ == y`, it is even indifferentiable!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(pred, y_test):\n",
    "    sum_err = 0.0\n",
    "    y = y_test.values\n",
    "    for i in range(len(y_test)):\n",
    "        sum_err += abs(pred[i] - y[i])\n",
    "    return(sum_err / float(len(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.538685478441911"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae(lr_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "註：還沒完全讀懂 https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Biased Error (MBE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mbe.png\" width=\"300\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Mean Biased Error`** is exceptionally useful in detecting the average model bias. \n",
    "\n",
    "Note that since positive and negative errors will cancel out (since the absolute sign is not taken), the error accounts for no variation and only bias. \n",
    "\n",
    "Although less accurate in practice, **`MBE`** can be used to determine whether the model can predict the intended result, or has positive or negative bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbe(pred, y_test):\n",
    "    sum_err = 0.0\n",
    "    y = y_test.values\n",
    "    for i in range(len(y_test)):\n",
    "        sum_err += pred[i] - y[i]\n",
    "    return(sum_err / float(len(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35401600164261304"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbe(lr_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Percentage Error (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Logarithmic Error  ( RMSLE )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rmsle2.png\" width=\"500\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With simple logarithmic calculation, we can easily detect that the main difference between `RMSLE` and `RMSE` lies in the fact that \n",
    "\n",
    "* `RMSLE` only considers the **relative error between and the Predicted and the actual value**. **The absolute difference (the scale of the error) is not significant**. On the other hand, `RMSE` increases in magnitude if the scale of error increases.\n",
    "\n",
    "\n",
    "* `RMSLE` penalizes predictions that are less than the actual values more than it penalized predictions more than actual values. (這邊可以想個例子來證明一下)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rmsleviz.png\" width=\"500\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommended Use Cases**:\n",
    "\n",
    "* **`When biased penalty is acceptable`** : `RMSLE` incurs a larger penalty for the `under-estimation` between predictions and actual values more than `over-estimation`.\n",
    "\n",
    "\n",
    "* **`When underestimated is not acceptable but overestimation can be allowed`** : `RMSLE` is especially useful for business cases where the underestimation of the target variable is not acceptable but overestimation can be tolerated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R² ( R-Squared / Coefficient of Determination )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that you get a MSE score of `5.21`. What should you do? Is this a decent value?\n",
    "\n",
    "To tackle this ambiguity, **`R²`** is a wonderful metric to evaluate how well the model fits the data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"r2.png\" width=\"250\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"r2-1.png\" width=\"300\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`MSE(model)`** : Sum squared Regression Error. This MSE is derived from whatever regression model we implemented.\n",
    "\n",
    "**`MSE(baseline)`** : Sum squared  Total Error. This constant baseline model can be interpreted as the **`simplest model`** we can derive -- which is to always predict the average of all samples.\n",
    "\n",
    "**`y̅`** : the mean of the observed yᵢ.\n",
    "\n",
    "**Simple Explanations**:\n",
    "\n",
    "1. **`R²`** is a scale-free metric and is widely used in terms of evaluating rooms for improvement for the model.\n",
    "2. **`R²`** is the ratio between **how good our model is** vs **how good is the naive mean model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric compares the fit of the chosen model with that of a horizontal straight line (the null hypothesis).\n",
    "\n",
    "A value close to `1` indicates that the modle perfectly has zero-to-none bias and variation (with close to zero error), and a value close to `0` indicates a model very close to the baseline.\n",
    "\n",
    "Note that  **`R²` can actually be negative**. **`R²`** is negative when the chosen model does not follow the trend of the data, so fits worse than a horizontal line. Therefore, **if the chosen model fits worse than a horizontal line, then 𝑅2 is negative**. \n",
    "\n",
    "Note that `R²` is not in fact the square of anything, so it can have a negative value without violating any rules of math. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"r2neg.png\" width=\"350\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted R² ( Adj. R-Squared )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the problem of common `R²`?\n",
    "\n",
    "`R²` suffers from the problem that the model could overfit the data. Imagine that you have five data points, and you derive the `R²` score of `0.70`. If you want to improve your `R²`, one simple way is to add one more variable into the model. \n",
    "\n",
    "Howver, the scores improve on increasing terms even though the model is exactly not improving. This potentially overfitting problem may misguide the researcher. In extreme cases, you can even get an `R²` of `1` when you have the same amounts of variables and data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"adjr2.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`p`** : Number of independent variables\n",
    "\n",
    "\n",
    "**`N`** : Number of observations (Sample Size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Adjusted R²`** is thus introduced to solve this problem.\n",
    "\n",
    "`Adjusted R²` is always lower than `R²` as it adjusts for the increasing predictors and only shows improvement if there is a real improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that there’s no one-size-fits-all loss function to all the machine learning problems. Each loss function performs well in respective use cases in detecting different model performances, and should be considered before implementing one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/regression-an-explanation-of-regression-metrics-and-what-can-go-wrong-a39a9793d914\n",
    "\n",
    "https://medium.com/@george.drakos62/how-to-select-the-right-evaluation-metric-for-machine-learning-models-part-1-regrression-metrics-3606e25beae0\n",
    "\n",
    "https://towardsdatascience.com/regression-an-explanation-of-regression-metrics-and-what-can-go-wrong-a39a9793d914\n",
    "\n",
    "https://stats.stackexchange.com/questions/12900/when-is-r-squared-negative"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
